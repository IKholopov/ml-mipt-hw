{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('task2_lemmas_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.drop(['y2', 'y3', 'y4', 'y5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['normed'] = data['y1'].apply(lambda x: x.split('+')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['part'] = data['y1'].apply(lambda x: x.split('+')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.drop(['y1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def transform_word(x):\n",
    "    x = [val.decode('utf-8') for val in x]\n",
    "    common = os.path.commonprefix(x)\n",
    "    ending  = x[0][len(common):]\n",
    "    normed_ending = x[1][len(common):]\n",
    "    return common, ending, normed_ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'vergogn', u'erete', u'are')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_word(list(data[['X', 'normed']].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_set = np.array([list(transform_word(list(word))) for word in data[['X', 'normed']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['common'] = transformed_set[:, 0]\n",
    "data['ending'] = transformed_set[:, 1]\n",
    "data['normed_ending'] = transformed_set[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that change the beginning 70 in 118640\n"
     ]
    }
   ],
   "source": [
    "print('Words that change the beginning {} in {}'.format(len(data[data['common']==u'']), len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data[~(data['common']==u'')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(data, 'preprocessed_words.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('preprocessed_words.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>X</th>\n",
       "      <th>normed</th>\n",
       "      <th>part</th>\n",
       "      <th>common</th>\n",
       "      <th>ending</th>\n",
       "      <th>normed_ending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>vergognerete</td>\n",
       "      <td>vergognare</td>\n",
       "      <td>V</td>\n",
       "      <td>vergogn</td>\n",
       "      <td>erete</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>amnistiavate</td>\n",
       "      <td>amnistiare</td>\n",
       "      <td>V</td>\n",
       "      <td>amnistia</td>\n",
       "      <td>vate</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>menomazione</td>\n",
       "      <td>menomazione</td>\n",
       "      <td>N</td>\n",
       "      <td>menomazione</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sfaldavamo</td>\n",
       "      <td>sfaldare</td>\n",
       "      <td>V</td>\n",
       "      <td>sfalda</td>\n",
       "      <td>vamo</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>sfodererei</td>\n",
       "      <td>sfoderare</td>\n",
       "      <td>V</td>\n",
       "      <td>sfoder</td>\n",
       "      <td>erei</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>ascondesti</td>\n",
       "      <td>ascondere</td>\n",
       "      <td>V</td>\n",
       "      <td>asconde</td>\n",
       "      <td>sti</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>edifichereste</td>\n",
       "      <td>edificare</td>\n",
       "      <td>V</td>\n",
       "      <td>edific</td>\n",
       "      <td>hereste</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>maschieran</td>\n",
       "      <td>maschiare</td>\n",
       "      <td>V</td>\n",
       "      <td>maschi</td>\n",
       "      <td>eran</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>transennasser</td>\n",
       "      <td>transennare</td>\n",
       "      <td>V</td>\n",
       "      <td>transenna</td>\n",
       "      <td>sser</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id              X       normed part       common   ending normed_ending\n",
       "0   1   vergognerete   vergognare    V      vergogn    erete           are\n",
       "1   2   amnistiavate   amnistiare    V     amnistia     vate            re\n",
       "2   3    menomazione  menomazione    N  menomazione                       \n",
       "3   4     sfaldavamo     sfaldare    V       sfalda     vamo            re\n",
       "4   5     sfodererei    sfoderare    V       sfoder     erei           are\n",
       "5   6     ascondesti    ascondere    V      asconde      sti            re\n",
       "6   7  edifichereste    edificare    V       edific  hereste           are\n",
       "7   8     maschieran    maschiare    V       maschi     eran           are\n",
       "8   9  transennasser  transennare    V    transenna     sser            re"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'N' 'V']\n"
     ]
    }
   ],
   "source": [
    "parts = np.unique(data['part'])\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique endings: 474\n",
      "Unique normed endings: 81\n"
     ]
    }
   ],
   "source": [
    "endings = np.unique(data['ending'])\n",
    "normed_endings = np.unique(data['normed_ending'])\n",
    "print('Unique endings: {}'.format(len(endings)))\n",
    "print('Unique normed endings: {}'.format(len(normed_endings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A & N: set([u'', u'a', u'e', u'io', u'o'])\n",
      "A & V: set([u'', u'e', u'ssere', u'rre', u're', u'ire'])\n",
      "V & N: set([u'', u'e'])\n"
     ]
    }
   ],
   "source": [
    "print('A & N: {}'.format(set(data[data['part']=='A']['normed_ending']) & set(data[data['part']=='N']['normed_ending'])))\n",
    "print('A & V: {}'.format(set(data[data['part']=='A']['normed_ending']) & set(data[data['part']=='V']['normed_ending'])))\n",
    "print('V & N: {}'.format(set(data[data['part']=='V']['normed_ending']) & set(data[data['part']=='N']['normed_ending'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'e-paga', 1), (u'dero', 1), (u'neremmo', 1), (u'e-lavoro', 1), (u'iedo', 1), (u'iedi', 1), (u'iede', 1), (u'der', 1), (u'ocessimo', 1), (u'ravamo', 1), (u'ocerebbe', 1), (u'\\xe8i', 1), (u'lero', 1), (u'i-quadro', 1), (u'cerai', 1), (u'usi', 1), (u'nerebbero', 1), (u'neremo', 1), (u'ocevan', 1), (u'uppero', 1), (u'ocesser', 1), (u'esorti', 1), (u'heforti', 1), (u'lta', 1), (u'nuti', 1), (u'nute', 1), (u'gliono', 1), (u'ito', 1), (u'ita', 1), (u'le', 1), (u'li', 1), (u'ociamo', 1), (u'i-leninismi', 1), (u'ces', 1), (u'he-dati', 1), (u'erte', 1), (u'ocete', 1), (u'erta', 1), (u'ider', 1), (u'neresti', 1), (u'nereste', 1), (u'ocevi', 1), (u'ssan', 1), (u'ro', 1), (u'ri', 1), (u'ocerebbero', 1), (u'uppe', 1), (u'lte', 1), (u'lti', 1), (u'lto', 1), (u'user', 1), (u'uoiano', 1), (u'uoiono', 1), (u'sson', 1), (u'i-spia', 1), (u'ssono', 1), (u'gliate', 1), (u'e-ordinanze', 1), (u'i-stati', 1), (u'esso', 1), (u'nei', 1), (u'i-paese', 1), (u'escono', 1), (u'di', 1), (u'de', 1), (u'nerono', 1), (u'edonne', 1), (u'nerei', 1), (u'iedevo', 1), (u'venti', 1), (u'esca', 1), (u'vente', 1), (u'reta', 1), (u'reto', 1), (u'esci', 1), (u'lga', 1), (u'e-gol', 1), (u'uoi', 1), (u'escon', 1), (u've', 1), (u'ini-radar', 1), (u'ssiate', 1), (u'e-partito', 1), (u'nerai', 1), (u'eforti', 1), (u'glian', 1), (u'uoio', 1), (u'cerebbe', 1), (u'i-italia', 1), (u'uoian', 1), (u'ravate', 1), (u'gliano', 1), (u'lgono', 1), (u'uori', 1), (u'-chiave', 1), (u'ide', 1), (u'ner\\xf2', 1), (u'oceva', 1), (u'ner\\xe0', 1), (u'ocevo', 1), (u'n\\xe8', 1), (u'imorte', 1), (u'oceran', 1), (u'glia', 1), (u'iclan', 1), (u'neron', 1), (u'iviri', 1), (u'esce', 1), (u'esco', 1), (u'ilista', 1), (u'nerebber', 1), (u'ipattuglia', 1), (u'escan', 1), (u'e-estati', 1), (u'u\\xf2', 1), (u'cian', 2), (u'ocereste', 2), (u'iedon', 2), (u'ieda', 2), (u'cer\\xe0', 2), (u'cion', 2), (u'ocer\\xe0', 2), (u'oceranno', 2), (u'igruppo', 2), (u'oceresti', 2), (u'use', 2), (u'usa', 2), (u'uso', 2), (u'ocendo', 2), (u'cereste', 2), (u'ceresti', 2), (u'etto', 2), (u'lgan', 2), (u'ocerete', 2), (u'ocesti', 2), (u'iti', 2), (u'oceste', 2), (u'ite', 2), (u'cerei', 2), (u'idero', 2), (u'ccian', 2), (u'i-chiave', 2), (u'erto', 2), (u'en', 2), (u'ei', 2), (u'ls\\xe8', 2), (u'cciano', 2), (u'rebber', 2), (u'ciute', 2), (u'ocemmo', 2), (u'ocer\\xf2', 2), (u'lsero', 2), (u'essa', 2), (u'ies', 2), (u'quer', 2), (u'lsi', 2), (u'ocevate', 2), (u'cerono', 2), (u'bbi', 2), (u'lgon', 2), (u'ocevamo', 2), (u'lgo', 2), (u'erti', 2), (u'oceremmo', 2), (u'ccia', 2), (u'oceremo', 2), (u'ocerei', 2), (u'quero', 2), (u'ociate', 2), (u'lser', 2), (u'ceremmo', 2), (u'lgano', 2), (u'idi', 2), (u'ocerai', 2), (u'iedono', 2), (u'cio', 2), (u'ocesse', 2), (u'cerebber', 2), (u'usero', 2), (u'tter', 2), (u'ocevano', 2), (u'rebbe', 3), (u'cerete', 3), (u'ver', 3), (u'vero', 3), (u'ceron', 3), (u'cer\\xf2', 3), (u'upper', 3), (u'nes', 3), (u'resti', 3), (u'ceran', 3), (u'etta', 3), (u'ette', 3), (u'iono', 3), (u'cei', 3), (u'rebbero', 3), (u'eci', 3), (u'que', 3), (u'qui', 3), (u'uppi', 3), (u'ciuta', 3), (u'ciuto', 3), (u'ciuti', 3), (u'cerebbero', 3), (u'ceremo', 3), (u'osso', 3), (u'ossa', 3), (u'ceranno', 3), (u'cciate', 3), (u'ciano', 3), (u'c\\xe8', 3), (u'io', 3), (u'remmo', 3), (u'cia', 3), (u'ciono', 3), (u'ini', 3), (u'evi', 4), (u'cciamo', 4), (u'ece', 4), (u'ecer', 4), (u'ete', 4), (u'remo', 4), (u'ccio', 4), (u'ion', 4), (u'rete', 4), (u'ecero', 4), (u'rei', 5), (u'osser', 5), (u'reste', 5), (u'iano', 5), (u'ran', 5), (u'etti', 5), (u'r\\xf2', 5), (u'r\\xe0', 5), (u'otta', 5), (u'isero', 5), (u'ttero', 5), (u'ranno', 5), (u'cete', 5), (u'ossero', 5), (u'evano', 5), (u'rai', 5), (u'ossi', 5), (u'ia', 5), (u'otto', 5), (u'otte', 5), (u'ggan', 5), (u'emmo', 5), (u'nno', 5), (u'eva', 6), (u'ner', 6), (u'gga', 6), (u'ggo', 6), (u'can', 6), (u'iene', 6), (u'evo', 6), (u'ian', 6), (u'ggano', 6), (u'ggono', 6), (u'este', 6), (u'osse', 6), (u'ggon', 6), (u'nevamo', 7), (u'essimo', 7), (u'ieni', 7), (u'evamo', 7), (u'esti', 7), (u'nente', 7), (u'essi', 7), (u'con', 7), (u'ca', 7), (u'otti', 7), (u'evate', 7), (u'evan', 7), (u'nenti', 8), (u'ss\\xe8', 8), (u'nessi', 8), (u'nevo', 8), (u'nero', 8), (u'ciamo', 8), (u'nendo', 8), (u'nevan', 8), (u'cesse', 9), (u'essero', 9), (u'rreste', 9), (u'isi', 9), (u'ise', 9), (u'ngono', 9), (u'nesse', 9), (u'niamo', 9), (u'sso', 9), (u'co', 9), (u'cono', 9), (u'rranno', 9), (u'cevamo', 9), (u'rrai', 9), (u'gano', 10), (u'nesti', 10), (u'neste', 10), (u'rr\\xe0', 10), (u'nemmo', 10), (u'iser', 10), (u'ngano', 10), (u'esse', 10), (u'ngo', 10), (u'niate', 10), (u'iuto', 10), (u'ci', 10), (u'cano', 10), (u'ce', 10), (u'rrete', 10), (u'rremmo', 10), (u'nessero', 11), (u'ngan', 11), (u'rrei', 11), (u'rremo', 11), (u'rresti', 11), (u'nessimo', 11), (u'nevano', 11), (u'ssa', 11), (u'nesser', 11), (u'ngon', 11), (u'cesti', 11), (u'rran', 11), (u'neva', 11), (u'nevi', 11), (u'rrebbe', 11), (u'nevate', 12), (u'rrebber', 12), (u'cevan', 12), (u'nete', 12), (u'cendo', 12), (u'cevate', 12), (u'rr\\xf2', 12), (u'cemmo', 12), (u'nga', 12), (u'rrebbero', 12), (u'esser', 12), (u'ga', 13), (u'go', 13), (u'centi', 13), (u'sta', 13), (u'ciate', 13), (u'iute', 13), (u'cesser', 13), (u'cevano', 13), (u'cessimo', 13), (u'ceste', 13), (u'ceva', 13), (u'gono', 13), (u'cessero', 13), (u'cessi', 14), (u'gon', 14), (u'sto', 14), (u'gan', 14), (u'cente', 14), (u'cevo', 14), (u'cevi', 15), (u'ni', 16), (u'ne', 16), (u'iuta', 17), (u'iuti', 18), (u'tto', 21), (u'tta', 23), (u's', 24), (u'tti', 25), (u'tte', 32), (u's\\xe8', 34), (u'sa', 35), (u'so', 38), (u'uti', 55), (u'ute', 58), (u'se', 59), (u'rici', 60), (u'uto', 61), (u'rice', 62), (u'uta', 70), (u'\\xe8', 74), (u'sci', 77), (u'sce', 77), (u'sco', 80), (u'ser', 80), (u'si', 81), (u'scon', 82), (u'sero', 82), (u'scano', 83), (u'scan', 86), (u'scono', 91), (u'sca', 92), (u'hin', 160), (u'hiate', 162), (u'hino', 167), (u'herete', 169), (u'amo', 170), (u'heranno', 170), (u'hereste', 171), (u'heremmo', 171), (u'hiamo', 171), (u'herei', 172), (u'her\\xe0', 174), (u'heremo', 175), (u'her\\xf2', 176), (u'heran', 176), (u'\\xec', 176), (u'herebbero', 179), (u'endo', 179), (u'heresti', 181), (u'enti', 181), (u'herai', 181), (u'ate', 184), (u'herebbe', 184), (u'herebber', 186), (u'ente', 193), (u'ano', 271), (u'hi', 281), (u'ai', 380), (u'bber', 384), (u'bbe', 389), (u'\\xe0', 392), (u'bbero', 398), (u'anno', 399), (u'he', 516), (u'mo', 649), (u'an', 661), (u'ino', 1082), (u'in', 1109), (u'iate', 1304), (u'iamo', 1307), (u'eran', 1352), (u'eremo', 1356), (u'erebbe', 1358), (u'eresti', 1359), (u'eremmo', 1373), (u'erebbero', 1380), (u'erai', 1383), (u'erete', 1384), (u'erebber', 1389), (u'erei', 1391), (u'ereste', 1396), (u'er\\xe0', 1400), (u'eranno', 1412), (u'er\\xf2', 1424), (u'ti', 1677), (u'to', 1687), (u'nte', 1697), (u'nti', 1712), (u'ta', 1720), (u'o', 1726), (u'ndo', 1755), (u'n', 1794), (u'no', 1827), (u'van', 1894), (u'vi', 1934), (u'vano', 1934), (u'sse', 1945), (u'vate', 1949), (u'vo', 1954), (u'ssimo', 1957), (u'\\xf2', 1959), (u'vamo', 1973), (u'ssi', 1974), (u'va', 1974), (u'sser', 1982), (u'ssero', 2016), (u'on', 2047), (u'ono', 2057), (u'a', 2080), (u'te', 2291), (u'ste', 2296), (u'sti', 2314), (u'mmo', 2326), (u'e', 2801), (u'i', 8636), (u'', 14302)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "counts = defaultdict(int)\n",
    "for ending in endings:\n",
    "    counts[ending] = np.sum(data['ending']==ending)\n",
    "print(sorted(counts.items(), key=operator.itemgetter(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'a-italia', 1), (u'e-quadro', 1), (u'a-paese', 1), (u'asorte', 1), (u'e-spia', 1), (u'adonna', 1), (u'oclan', 1), (u'a-paga', 1), (u'a-lavoro', 1), (u'to', 1), (u'oprassedere', 1), (u'si', 1), (u'opattuglia', 1), (u'o-leninismo', 1), (u'o-radar', 1), (u'a-partito', 1), (u'otere', 1), (u'a-dati', 1), (u'o-stato', 1), (u'omorta', 1), (u'a-ordinanza', 1), (u'olere', 1), (u'a-gol', 1), (u'a-estate', 1), (u'oviro', 1), (u'x', 1), (u'olista', 1), (u'ogruppo', 2), (u'y', 2), (u'aforte', 2), (u'an', 2), (u'o-chiave', 3), (u'ernere', 3), (u'ingere', 3), (u'ssere', 5), (u'orire', 5), (u'tere', 5), (u'enere', 6), (u'scere', 7), (u'igere', 7), (u'enire', 7), (u'uscire', 7), (u'guere', 7), (u'ompere', 8), (u'uotere', 8), (u'ondere', 10), (u'mpere', 10), (u'uovere', 11), (u'imere', 11), (u'edere', 15), (u'arsi', 15), (u'urre', 16), (u'rsi', 17), (u'rire', 17), (u'io', 20), (u'rere', 22), (u'gliere', 22), (u'vere', 23), (u'cere', 25), (u'ia', 26), (u'ttere', 27), (u'ettere', 33), (u'lere', 43), (u'nere', 60), (u'nire', 60), (u'uocere', 63), (u'ggere', 79), (u'dere', 79), (u'ore', 122), (u'ndere', 175), (u'gere', 179), (u'rre', 829), (u'ire', 1378), (u'a', 1605), (u'iare', 1765), (u'ere', 2216), (u'o', 7189), (u'e', 9493), (u'', 13383), (u'are', 30125), (u're', 49291)]\n",
      "defaultdict(<type 'int'>, {u'': 14302, u'sco': 80, u'sci': 77, u'sce': 77, u'cian': 2, u'sca': 92, u'e-paga': 1, u'dero': 1, u'ga': 13, u'go': 13, u'rebbe': 3, u'gano': 10, u'cerete': 3, u'ocereste': 2, u'ver': 3, u'neremmo': 1, u'nevate': 12, u'to': 1687, u'rei': 5, u'heresti': 181, u'hereste': 171, u'ta': 1720, u'iamo': 1307, u'her\\xe0': 174, u'osser': 5, u'e-lavoro': 1, u'iedo': 1, u'iedon': 2, u'iedi': 1, u'her\\xf2': 176, u'iede': 1, u'vero': 3, u'ieda': 2, u'evi': 4, u'cer\\xe0': 2, u'cion': 2, u'eva': 6, u'ceron': 3, u'cer\\xf2': 3, u'der': 1, u'upper': 3, u'ente': 193, u'nesti': 10, u'enti': 181, u'neste': 10, u'ocessimo': 1, u'ocer\\xe0': 2, u'ravamo': 1, u'oceranno': 2, u'herete': 169, u'herei': 172, u'igruppo': 2, u'ocerebbe': 1, u'ner': 6, u'nes': 3, u'nevamo': 7, u'cciamo': 4, u'\\xe8i': 1, u'gga': 6, u'lero': 1, u'ggo': 6, u'i-quadro': 1, u'\\xe0': 392, u'resti': 3, u'reste': 5, u'ceran': 3, u'cerai': 1, u'iano': 5, u'herebber': 186, u'ran': 5, u'oceresti': 2, u'use': 2, u'usa': 2, u'uso': 2, u'usi': 1, u'vi': 1934, u'essimo': 7, u'nerebbero': 1, u'neremo': 1, u'ti': 1677, u'ocendo': 2, u'rrebber': 12, u'cereste': 2, u'ocevan': 1, u'vo': 1954, u'ceresti': 2, u'uppero': 1, u'te': 2291, u'mo': 649, u'centi': 13, u'cevan': 12, u'nessero': 11, u's\\xe8': 34, u'can': 6, u'heran': 176, u'etti': 5, u'etto': 2, u'herai': 181, u'scan': 86, u'rr\\xe0': 10, u'etta': 3, u'nemmo': 10, u'ette': 3, u'ocesser': 1, u'esorti': 1, u'ngan': 11, u'heforti': 1, u'iono': 3, u'sser': 1982, u'lta': 1, u'lgan': 2, u'nuti': 1, u'eranno': 1412, u'nute': 1, u'cesse': 9, u'gliono': 1, u'cessi': 14, u'ocerete': 2, u'a': 2080, u'er\\xf2': 1424, u'essero': 9, u'ieni': 7, u'si': 81, u'er\\xe0': 1400, u'sa': 35, u'iene': 6, u'se': 59, u'ito': 1, u'ocesti': 2, u'erete': 1384, u'iti': 2, u'oceste': 2, u'nete': 12, u'ite': 2, u'ita': 1, u'tti': 25, u'tto': 21, u'cerei': 2, u'tta': 23, u'tte': 32, u'le': 1, u'idero': 2, u'r\\xf2': 5, u'ccian': 2, u'scon': 82, u'li': 1, u'r\\xe0': 5, u'ociamo': 1, u'i-chiave': 2, u'iser': 10, u'cendo': 12, u'rrei': 11, u'i-leninismi': 1, u'cei': 3, u'otta': 5, u'evamo': 7, u'ces': 1, u'isero': 5, u'evo': 6, u'he-dati': 1, u'erto': 2, u'en': 2, u'ei': 2, u'ttero': 5, u'ranno': 5, u'erte': 1, u'ocete': 1, u'erta': 1, u'ls\\xe8': 2, u'cciano': 2, u'cete': 5, u'rebbero': 3, u'eremo': 1356, u'ider': 1, u'neresti': 1, u'gon': 14, u'nereste': 1, u'ocevi': 1, u'ossero': 5, u'herebbero': 179, u'ssan': 1, u'ro': 1, u'ri': 1, u'rebber': 2, u'ece': 4, u'ocerebbero': 1, u'rreste': 9, u'rremo': 11, u'rresti': 11, u'eci': 3, u'nti': 1712, u'mmo': 2326, u'scano': 83, u'que': 3, u'uppe': 1, u'qui': 3, u'uppi': 3, u'nte': 1697, u'ciute': 2, u'ate': 184, u'ciuta': 3, u'ciuto': 3, u'ciuti': 3, u'cevate': 12, u'isi': 9, u'ecer': 4, u'lte': 1, u'lti': 1, u'lto': 1, u'ise': 9, u'user': 1, u'ono': 2057, u'ocemmo': 2, u'nessimo': 11, u'evano': 5, u'ser': 80, u'ian': 6, u'ggano': 6, u'rr\\xf2': 12, u'uoiano': 1, u'ocer\\xf2': 2, u'ggono': 6, u'uoiono': 1, u'sson': 1, u'ste': 2296, u'sta': 13, u'i-spia': 1, u'sto': 14, u'sti': 2314, u'ssono': 1, u'nevano': 11, u'heremmo': 171, u'sero': 82, u'gliate': 1, u'este': 6, u'cemmo': 12, u'ano': 271, u'esti': 7, u'rai': 5, u'ssimo': 1957, u'e-ordinanze': 1, u'cerebbero': 3, u'ngono': 9, u'scono': 91, u'i-stati': 1, u'nente': 7, u'so': 38, u'nenti': 8, u'lsero': 2, u'ngano': 10, u'ssero': 2016, u'bber': 384, u'essa': 2, u'esse': 10, u'essi': 7, u'esso': 1, u'nesse': 9, u'hiate': 162, u'nei': 1, u'\\xf2': 1959, u'i-paese': 1, u'ceremo': 3, u'nga': 12, u'ies': 2, u'ngo': 10, u'ereste': 1396, u'osso': 3, u'quer': 2, u'ossi': 5, u'rici': 60, u'escono': 1, u'osse': 6, u'rice': 62, u'ossa': 3, u'niamo': 9, u'di': 1, u'de': 1, u'iate': 1304, u'sso': 9, u'ssi': 1974, u'sse': 1945, u'lsi': 2, u'ssa': 11, u'hin': 160, u'nerono': 1, u'ciate': 13, u'eresti': 1359, u'hiamo': 171, u'edonne': 1, u'ceranno': 3, u'nerei': 1, u'iedevo': 1, u'ocevate': 2, u'heremo': 175, u'erei': 1391, u'cciate': 3, u'venti': 1, u'ss\\xe8': 8, u'endo': 179, u'niate': 10, u'\\xe8': 74, u'iuto': 10, u's': 24, u'ete': 4, u'iute': 13, u'con': 7, u'esca': 1, u'ci': 10, u'co': 9, u'ca': 7, u'ciano': 3, u'nesser': 11, u'cerono': 2, u'vente': 1, u'reta': 1, u'bbi': 2, u'bbe': 389, u'reto': 1, u'esci': 1, u'lgon': 2, u'ocevamo': 2, u'lgo': 2, u'lga': 1, u'e-gol': 1, u'gan': 14, u'uoi': 1, u'vano': 1934, u'cesser': 13, u'cevano': 13, u'n': 1794, u'escon': 1, u'remo': 4, u'va': 1974, u'c\\xe8': 3, u've': 1, u'eran': 1352, u'cano': 10, u'cessimo': 13, u'erai': 1383, u'erti': 2, u'io': 3, u'in': 1109, u'ia': 5, u'erebbe': 1358, u'cente': 14, u'cono': 9, u'oceremmo': 2, u'ini-radar': 1, u'ssiate': 1, u'otti': 7, u'e-partito': 1, u'otto': 5, u'nerai': 1, u'otte': 5, u'evate': 7, u'rrebbero': 12, u'ccia': 2, u'oceremo': 2, u'iuti': 18, u'ccio': 4, u'ion': 4, u'eforti': 1, u'herebbe': 184, u'i': 8636, u'nessi': 8, u'iuta': 17, u'glian': 1, u'vamo': 1973, u'ngon': 11, u'ocerei': 2, u'ggan': 5, u'ceste': 13, u'rranno': 9, u'hino': 167, u'cesti': 11, u'amo': 170, u'heranno': 170, u'cevamo': 9, u'ce': 10, u'quero': 2, u'ociate': 2, u'emmo': 5, u'lser': 2, u'uoio': 1, u'rrai': 9, u'rran': 11, u'neva': 11, u'vate': 1949, u'cerebbe': 1, u'nevo': 8, u'nevi': 11, u'i-italia': 1, u'ceremmo': 2, u'anno': 399, u'eremmo': 1373, u'uoian': 1, u'lgano': 2, u'ravate': 1, u'gliano': 1, u'lgono': 1, u'on': 2047, u'\\xec': 176, u'uori': 1, u'rete': 4, u'ndo': 1755, u'erebber': 1389, u'erebbero': 1380, u'o': 1726, u'-chiave': 1, u'idi': 2, u'remmo': 3, u'ide': 1, u'esser': 12, u'ner\\xf2': 1, u'rrete': 10, u'ceva': 13, u'oceva': 1, u'ner\\xe0': 1, u'ocevo': 1, u'cevo': 14, u'n\\xe8': 1, u'imorte': 1, u'van': 1894, u'rrebbe': 11, u'ocerai': 2, u'oceran': 1, u'glia': 1, u'iclan': 1, u'rremmo': 10, u'hi': 281, u'iedono': 2, u'neron': 1, u'he': 516, u'cia': 3, u'uta': 70, u'iviri': 1, u'ute': 58, u'uti': 55, u'cio': 2, u'uto': 61, u'ocesse': 2, u'ecero': 4, u'nno': 5, u'ciono': 3, u'esce': 1, u'ai': 380, u'gono': 13, u'esco': 1, u'an': 661, u'ino': 1082, u'ilista': 1, u'ini': 3, u'nero': 8, u'cessero': 13, u'ni': 16, u'no': 1827, u'cevi': 15, u'nerebber': 1, u'ipattuglia': 1, u'ne': 16, u'ciamo': 8, u'ggon': 6, u'cerebber': 2, u'nendo': 8, u'evan': 7, u'usero': 2, u'nevan': 8, u'tter': 2, u'e': 2801, u'escan': 1, u'e-estati': 1, u'u\\xf2': 1, u'bbero': 398, u'ocevano': 2})\n"
     ]
    }
   ],
   "source": [
    "counts_normed = defaultdict(int)\n",
    "for ending in normed_endings:\n",
    "    counts_normed[ending] = np.sum(data['normed_ending']==ending)\n",
    "print(sorted(counts_normed.items(), key=operator.itemgetter(1)))\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим бор для быстрого поиска подходящих окончаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_trie(endings):\n",
    "    root = {}\n",
    "    for l in range(1, np.max([len(end) for end in endings])+1):\n",
    "        for end in endings:\n",
    "            if len(end) == l: \n",
    "                cur = root\n",
    "                for i in reversed(range(l)):\n",
    "                    if end[i] not in cur.keys():\n",
    "                        cur[end[i]] = {}\n",
    "                    cur = cur[end[i]]\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endings_trie = build_trie(endings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normed_endings_trie = build_trie(normed_endings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endings_dict = {}\n",
    "endings_dict_rev = {}\n",
    "for i in range(len(endings)):\n",
    "    endings_dict[endings[i]] = i\n",
    "    endings_dict_rev[i] = endings[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем список возможных окончаний по слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordendings(word):\n",
    "    vec = []\n",
    "    seq = u''\n",
    "    vec.append(seq)\n",
    "    word=word.decode('utf-8')\n",
    "    for i in reversed(range(len(word))):\n",
    "        seq = word[i] + seq\n",
    "        if seq[len(seq)-1] in endings_trie.keys():\n",
    "            if seq in endings:\n",
    "                vec.append(seq)\n",
    "        else:\n",
    "            break\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодируем возможные окончания слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordendings2vec(word):\n",
    "    vec = np.zeros(len(endings))\n",
    "    vec[endings_dict[u'']] = 1.0\n",
    "    seq = u''\n",
    "    word=word.decode('utf-8')\n",
    "    for i in reversed(range(len(word))):\n",
    "        seq = word[i] + seq\n",
    "        if seq[len(seq)-1] in endings_trie.keys():\n",
    "            if seq in endings:\n",
    "                vec[endings_dict[seq]] = 1.0\n",
    "        else:\n",
    "            break\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'', u'e', u'te', u'ete', u'rete', u'erete']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordendings(data['X'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded = np.array([wordendings2vec(word) for word in data['X'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = np.array([endings_dict[val] for val in data['ending'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbols = np.unique(list(''.join([word.decode('utf-8') for word in data['X'].values])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим по линейной регрессии для каждого окончания, по последним буквам корня, в словах, где эти окончания допустимы. В качестве ответа для слова будем брать слово, классификатор которого показал наибольшую вероятность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_gr_length = 4\n",
    "ind = 0\n",
    "grams_dict = defaultdict(int)\n",
    "for w in data['common'].values:\n",
    "    for i in range(n_gr_length):\n",
    "        if len(w) > i:\n",
    "            grams_dict[w[-i-1:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_lengths = 2000\n",
    "grams_dict_ind = {}\n",
    "ind = 0\n",
    "for p in sorted(grams_dict.items(), key=lambda x:x[1])[-f_lengths:]:\n",
    "    grams_dict_ind[p[0]] = ind\n",
    "    ind += 1\n",
    "grams_dict = grams_dict_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_predicessors(words, ending):\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        word = word.decode('utf-8')\n",
    "        vec = np.zeros(f_lengths)\n",
    "        for i in range(n_gr_length):\n",
    "            if len(ending) == 0 and word[-len(ending) - 1 - i:] in grams_dict.keys() :\n",
    "                    vec[grams_dict[word[-len(ending) - 1 - i:]]] = 1\n",
    "                    \n",
    "            elif word[-len(ending) - 1 - i:-len(ending)] in grams_dict.keys():\n",
    "                vec[grams_dict[word[-len(ending) - 1 - i:-len(ending)]]] = 1\n",
    "        vecs.append(vec)\n",
    "    return np.array(vecs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_classifiers = {}\n",
    "for ending in endings:\n",
    "    words = data[encoded[:,endings_dict[ending]] == 1][['X', 'ending']]\n",
    "    X = extract_predicessors(words['X'], ending)\n",
    "    y = (words['ending'] == ending)\n",
    "    if(all(y)):\n",
    "            ending_classifiers[ending] = 1\n",
    "            continue\n",
    "    clf = LogisticRegression(C=100)\n",
    "    clf.fit(X, y)\n",
    "    ending_classifiers[ending] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def detect_ending(X):\n",
    "    endings_lists = [get_wordendings(word) for word in X]\n",
    "    results = []\n",
    "    j = 0\n",
    "    for i in range(len(endings_lists)):\n",
    "        probs = {}\n",
    "        if i % (len(endings_lists)/10) == 0:\n",
    "            print(j)\n",
    "            j+=10\n",
    "        for end in endings_lists[i]:\n",
    "            if ending_classifiers[end] == 1:\n",
    "                probs[end] = 1\n",
    "                continue\n",
    "            probs[end]=ending_classifiers[end].predict_proba(extract_predicessors([X[i]], end))[0][1]\n",
    "        results.append(max(probs.iteritems(), key=operator.itemgetter(1))[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "y_pred=detect_ending(data['X'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93463776672008092"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data['ending'].values == y_pred)/float(len(y_pred)) #C=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93463776672008092"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data['ending'].values == y_pred)/float(len(y_pred)) #C=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим для каждого окончания в нормальной форме также по регрессору - фичами будут окончания и последние буквы корня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_ending = []\n",
    "for enc in data['ending'].values:\n",
    "    vec = np.zeros(len(endings))\n",
    "    vec[endings_dict[enc]] += 1\n",
    "    enc_ending.append(vec)\n",
    "enc_ending = np.array(enc_ending)\n",
    "encs_pred = extract_predicessors([word.encode('utf-8') for word in data['common'].values], u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_ending = np.hstack((enc_ending, encs_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "norm_end_classifiers = {}\n",
    "j=0\n",
    "for i in range(len(normed_endings)):\n",
    "    if i % (len(normed_endings)/10) == 0:\n",
    "            print(j)\n",
    "            j+=10\n",
    "    clf = LogisticRegression(C=100)\n",
    "    clf.fit(enc_ending, data['normed_ending']==normed_endings[i])\n",
    "    norm_end_classifiers[normed_endings[i]] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_endings(X):\n",
    "    results = []\n",
    "    encs = []\n",
    "    for enc in X['ending'].values:\n",
    "        vec = np.zeros(len(endings))\n",
    "        vec[endings_dict[enc]] += 1\n",
    "        encs.append(vec)\n",
    "    encs = np.hstack((encs, extract_predicessors([word.encode('utf-8') for word in X['common'].values], u'')))\n",
    "    probs = []\n",
    "    for norm in normed_endings:\n",
    "         probs.append(norm_end_classifiers[norm].predict_proba(encs)[:,1])\n",
    "    probs = (np.transpose(probs))\n",
    "    results = [normed_endings[np.argmax(prob)] for prob in probs]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred_normed=get_new_endings(data[['ending', 'common']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97182255207894075"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data['normed_ending'].values == y_pred_normed)/float(len(y_pred_normed)) #C=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_classifiers = {}\n",
    "j=0\n",
    "for i in range(len(parts)):\n",
    "    clf = LogisticRegression(C=100)\n",
    "    clf.fit(enc_ending, data['part']==parts[i])\n",
    "    part_classifiers[parts[i]] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(X):\n",
    "    results = []\n",
    "    encs = []\n",
    "    for enc in X['ending'].values:\n",
    "        vec = np.zeros(len(endings))\n",
    "        vec[endings_dict[enc]] += 1\n",
    "        encs.append(vec)\n",
    "    encs = np.hstack((encs, extract_predicessors([word.encode('utf-8') for word in X['common'].values], u'')))\n",
    "    probs = []\n",
    "    for part in parts:\n",
    "         probs.append(part_classifiers[part].predict_proba(encs)[:,1])\n",
    "    probs = (np.transpose(probs))\n",
    "    results = [parts[np.argmax(prob)] for prob in probs]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_parts=get_parts(data[['ending', 'common']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95694526440077587"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data['part'].values == y_pred_parts)/float(len(y_pred_parts)) #C=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('task2_lemmas_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "y_pred=detect_ending(test_data['X'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['ending']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common = []\n",
    "for i in range(len(test_data)):\n",
    "    if len(test_data['ending'].values[i]) == 0:\n",
    "            common.append(test_data['X'].values[i].decode('utf-8'))\n",
    "            continue\n",
    "    common.append(test_data['X'].values[i].decode('utf-8')[:-len(test_data['ending'].values[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['common'] = common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_normed = get_new_endings(test_data[['ending', 'common']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['normed_ending'] = y_pred_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_parts=get_parts(test_data[['ending', 'common']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['part'] = y_pred_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('task_2_subm2', 'w', 'utf-8') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i in range(len(test_data)):\n",
    "        f.write(u'{},'.format(i+1) + test_data['common'].values[i]+test_data['normed_ending'].values[i] +u'+'+test_data['part'].values[i] +u'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
